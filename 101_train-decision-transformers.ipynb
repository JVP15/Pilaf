{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DktITQNXTopc",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "from preprocessed_dataset import DecisionTransformerPreprocessedDataset, UnwrapCollator\n",
    "import torch.utils.data\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from decision_transformer import DecisionTransformerConfig, DecisionTransformerModel\n",
    "from dt_backgammon_env import RandomAgent, TDAgent, DTAgent, DecisionTransformerBackgammonEnv, BLACK, WHITE\n",
    "\n",
    "import snowietxt_processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 120\n",
    "LOG_DIR = os.path.join('saved_models', 'decision_transformer', datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFmTdHoHUD13"
   },
   "source": [
    "### Step 4: Defining a custom DataCollator for the transformers Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.46file/s, file=intermediate_vs_intermediate_25_36.txt]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<00:00, 5000.96it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = snowietxt_processor.create_dataset(num_files=100)\n",
    "dataset = datasets.Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<?, ?it/s]\n",
      "Preprocessing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  2.10batch/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = DecisionTransformerPreprocessedDataset(dataset, max_len=MAX_LEN, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmTRGPKYUVFG"
   },
   "source": [
    "### Step 5: Extending the Decision Transformer Model to include a loss function\n",
    "\n",
    "In order to train the model with the ðŸ¤— trainer class, we first need to ensure the dictionary it returns contains a loss, in this case L-2 norm of the models action predictions and the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zIJCY3b3pQAh"
   },
   "outputs": [],
   "source": [
    "config = DecisionTransformerConfig(state_dim=dataset.state_dim, act_dim=dataset.act_dim, max_length=MAX_LEN)\n",
    "model = DecisionTransformerModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJJ2mr_cU4eE"
   },
   "source": [
    "### Step 6: Defining the training hyperparameters and training the model\n",
    "Here, we define the training hyperparameters and our Trainer class that we'll use to train our Decision Transformer model.\n",
    "\n",
    "This step takes about an hour, so you may leave it running. Note the authors train for at least 3 hours, so the results presented here are not as performant as the models hosted on the ðŸ¤— hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateModelCallback(transformers.integrations.TensorBoardCallback):\n",
    "    def __init__(self, model, num_episodes):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_episodes = num_episodes\n",
    "\n",
    "        self.first_log = False # there are some scalars that we only want to log once, but we don't want to log them until the first time we log\n",
    "\n",
    "        self.random_agent = RandomAgent(BLACK)\n",
    "        self.beginner_agent = TDAgent(BLACK, 'beginner')\n",
    "        self.intermediate_agent = TDAgent(BLACK, 'intermediate')\n",
    "\n",
    "        self.dt_agent = DTAgent(WHITE, self.model)\n",
    "\n",
    "        self.backgammon_env = DecisionTransformerBackgammonEnv()\n",
    "\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        super().on_log(args, state, control, logs, **kwargs)\n",
    "\n",
    "        if not self.first_log:\n",
    "            # log the number of episodes we're evaluating on\n",
    "            self.tb_writer.add_scalar(\"eval/num_episodes\", self.num_episodes, 0)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # log the number of games won by the decision transformer agent\n",
    "        wins_random = self.backgammon_env.evaluate_agents({WHITE: self.dt_agent, BLACK: self.random_agent}, self.num_episodes, verbose=0)[WHITE]\n",
    "        wins_beginner = self.backgammon_env.evaluate_agents({WHITE: self.dt_agent, BLACK: self.beginner_agent}, self.num_episodes, verbose=0)[WHITE]\n",
    "        wins_intermediate = self.backgammon_env.evaluate_agents({WHITE: self.dt_agent, BLACK: self.intermediate_agent}, self.num_episodes, verbose=0)[WHITE]\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        self.tb_writer.add_scalar(\"eval/wins/random\", wins_random, state.epoch)\n",
    "        self.tb_writer.add_scalar(\"eval/wins/beginner\", wins_beginner, state.epoch)\n",
    "        self.tb_writer.add_scalar(\"eval/wins/intermediate\", wins_intermediate, state.epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNzzKWuuU9I4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\jorda\\.conda\\envs\\Perceiver\\lib\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7\n",
      "  Num Epochs = 120\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 840\n",
      "  Number of trainable parameters = 1349795\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='841' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [840/840 00:09, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\decision_transformer\\2023-05-16-18-59-30\\checkpoint-500\n",
      "Configuration saved in saved_models\\decision_transformer\\2023-05-16-18-59-30\\checkpoint-500\\config.json\n",
      "Model weights saved in saved_models\\decision_transformer\\2023-05-16-18-59-30\\checkpoint-500\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    "    tf32=True,\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_steps=2000,\n",
    "    output_dir=LOG_DIR\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=UnwrapCollator(),\n",
    ")\n",
    "\n",
    "# remove the old tensorboard trainer callback\n",
    "trainer.remove_callback(transformers.integrations.TensorBoardCallback)\n",
    "# add our own tensorboard/evaluation callback\n",
    "trainer.add_callback(EvaluateModelCallback(model, 20))\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
