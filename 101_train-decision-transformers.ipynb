{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DktITQNXTopc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "from preprocessed_dataset import DecisionTransformerPreprocessedDataset, UnwrapCollator\n",
    "import torch.utils.data\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from decision_transformer import DecisionTransformerConfig, DecisionTransformerModel\n",
    "\n",
    "import snowietxt_processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFmTdHoHUD13"
   },
   "source": [
    "### Step 4: Defining a custom DataCollator for the transformers Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening games\\game0\n",
      "Opening games\\game1\n",
      "Opening games\\game2\n",
      "Opening games\\game3\n",
      "Opening games\\game4\n",
      "Opening games\\game5\n",
      "Opening games\\game6\n",
      "Opening games\\game7\n",
      "Opening games\\game8\n",
      "Opening games\\game9\n",
      "Opening games\\game10\n",
      "Opening games\\game11\n",
      "Opening games\\game12\n",
      "Opening games\\game13\n",
      "Opening games\\game14\n",
      "Opening games\\game15\n",
      "Opening games\\game16\n",
      "Opening games\\game17\n",
      "Opening games\\game18\n",
      "Opening games\\game19\n",
      "Opening games\\game20\n",
      "Opening games\\game21\n",
      "Opening games\\game22\n",
      "Opening games\\game23\n",
      "Opening games\\game24\n",
      "Opening games\\game25\n",
      "Opening games\\game26\n",
      "Opening games\\game27\n",
      "Opening games\\game28\n",
      "Opening games\\game29\n",
      "Opening games\\game30\n",
      "Opening games\\game31\n",
      "Opening games\\game32\n",
      "Opening games\\game33\n",
      "Opening games\\game34\n",
      "Opening games\\game35\n",
      "Opening games\\game36\n",
      "Opening games\\game37\n",
      "Opening games\\game38\n",
      "Opening games\\game39\n",
      "Opening games\\game40\n",
      "Opening games\\game41\n",
      "Opening games\\game42\n",
      "Opening games\\game43\n",
      "Opening games\\game44\n",
      "Opening games\\game45\n",
      "Opening games\\game46\n",
      "Opening games\\game47\n",
      "Opening games\\game48\n",
      "Opening games\\game49\n",
      "Opening games\\game50\n",
      "Opening games\\game51\n",
      "Opening games\\game52\n",
      "Opening games\\game53\n",
      "Opening games\\game54\n",
      "Opening games\\game55\n",
      "Opening games\\game56\n",
      "Opening games\\game57\n",
      "Opening games\\game58\n",
      "Opening games\\game59\n",
      "Number of games 5105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5105/5105 [00:02<00:00, 1981.27it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = snowietxt_processor.create_dataset()\n",
    "dataset = datasets.Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 350/480 [06:36<00:54,  2.38batch/s]"
     ]
    }
   ],
   "source": [
    "dataset = DecisionTransformerPreprocessedDataset(dataset, max_len=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to preprocess the dataset using the data collator we defined above (it also handles the batches\n",
    "# right now, just test to see how many batches we get using the collator and batch size of 64\n",
    "# from torch.utils.data import DataLoader\n",
    "#\n",
    "# collator = DecisionTransformerGymDataCollator(dataset)\n",
    "# dataloader = DataLoader(dataset, batch_size=64, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, actions, rewards, returns_to_go, timesteps, attention_mask = [], [], [], [], [], []\n",
    "#\n",
    "# num_batches = 0\n",
    "#\n",
    "# for i in range(10 // DecisionTransformerGymDataCollator.max_len): # TODO: figure out the average length of an episode\n",
    "#     for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "#         states.append(batch[\"states\"])\n",
    "#         actions.append(batch[\"actions\"])\n",
    "#         rewards.append(batch[\"rewards\"])\n",
    "#         returns_to_go.append(batch[\"returns_to_go\"])\n",
    "#         timesteps.append(batch[\"timesteps\"])\n",
    "#         attention_mask.append(batch[\"attention_mask\"])\n",
    "#\n",
    "#         num_batches += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how much memory the states take up\n",
    "# states_view = torch.cat(states, dim=0)\n",
    "# actions_view = torch.cat(actions, dim=0)\n",
    "#\n",
    "# print(f\"States shape: {states_view.shape}\")\n",
    "# print(f\"Actions shape: {actions_view.shape}\")\n",
    "#\n",
    "# # calculate how much memory the states take up\n",
    "# print(f\"States memory: {states_view.element_size() * states_view.nelement() / 1e6} MB\")\n",
    "# print(f\"Actions memory: {actions_view.element_size() * actions_view.nelement() / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmTRGPKYUVFG"
   },
   "source": [
    "### Step 5: Extending the Decision Transformer Model to include a loss function\n",
    "\n",
    "In order to train the model with the ðŸ¤— trainer class, we first need to ensure the dictionary it returns contains a loss, in this case L-2 norm of the models action predictions and the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIJCY3b3pQAh"
   },
   "outputs": [],
   "source": [
    "config = DecisionTransformerConfig(state_dim=dataset.state_dim, act_dim=dataset.act_dim)\n",
    "model = DecisionTransformerModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJJ2mr_cU4eE"
   },
   "source": [
    "### Step 6: Defining the training hyperparameters and training the model\n",
    "Here, we define the training hyperparameters and our Trainer class that we'll use to train our Decision Transformer model.\n",
    "\n",
    "This step takes about an hour, so you may leave it running. Note the authors train for at least 3 hours, so the results presented here are not as performant as the models hosted on the ðŸ¤— hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNzzKWuuU9I4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    "    tf32=True,\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=UnwrapCollator(),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create a dataloader for evaluation\n",
    "eval_dataloader = DataLoader(dataset, batch_size=1, collate_fn=dummy_collator)\n",
    "\n",
    "# get one batch from the dataloader and run it through the model\n",
    "batch = next(iter(eval_dataloader))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.original_forward(**batch)\n",
    "\n",
    "print(output['action_preds'][0].argmax(dim=-1))\n",
    "print(batch['actions'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
